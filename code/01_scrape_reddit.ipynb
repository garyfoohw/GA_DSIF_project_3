{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79a3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d8d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API base url\n",
    "baseurl=\"https://api.pushshift.io/reddit/search/submission?size=100&subreddit=\"\n",
    "#topics to scrape for\n",
    "subreddit=[\"atheism\",\"christianity\"]\n",
    "#mininimum word count per post\n",
    "word_count_threshold=20\n",
    "#minimum posts to scrape for\n",
    "min_posts_required=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99465128",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_scrapes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116e660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(js,current_count,continue_flag,b_url,ss):\n",
    "    \"\"\"\n",
    "    A function which checks how many valid posts are there in the scraped JSON,\n",
    "    and finds the earliest date to pass to the next scrape.\n",
    "    \"\"\"\n",
    "    valid_posts_retrieved=current_count\n",
    "    data=js['data']\n",
    "    \n",
    "    for l in range(len(js['data'])):\n",
    "        try:\n",
    "            selftext=data[l]['selftext']\n",
    "            len_selftext=len(selftext.split())\n",
    "\n",
    "            if len_selftext >= word_count_threshold:\n",
    "                valid_posts_retrieved+=1\n",
    "                ss.append(selftext)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if valid_posts_retrieved>min_posts_required:\n",
    "        continue_flag=False\n",
    "    else:\n",
    "        continue_flag=True\n",
    "    \n",
    "    last_epoch=data[-1]['created_utc']\n",
    "    url=f'{b_url}{last_epoch}'      \n",
    "    \n",
    "    print(f\"Posts obtained: {valid_posts_retrieved}\")\n",
    "    \n",
    "    return valid_posts_retrieved, continue_flag, url, saved_scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567a57c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Running subreddit: atheism\n",
      "Posts obtained: 22\n",
      "Posts obtained: 74\n",
      "Posts obtained: 126\n",
      "Posts obtained: 178\n",
      "Posts obtained: 222\n",
      "Posts obtained: 261\n",
      "Posts obtained: 312\n",
      "Posts obtained: 359\n",
      "Posts obtained: 396\n",
      "Posts obtained: 444\n",
      "Posts obtained: 500\n",
      "Posts obtained: 557\n",
      "Posts obtained: 599\n",
      "Posts obtained: 641\n",
      "Posts obtained: 683\n",
      "Posts obtained: 725\n",
      "Posts obtained: 775\n",
      "Posts obtained: 827\n",
      "Posts obtained: 878\n",
      "Posts obtained: 924\n",
      "Posts obtained: 971\n",
      "Posts obtained: 1016\n",
      "Posts obtained: 1066\n",
      "Posts obtained: 1110\n",
      "Posts obtained: 1160\n",
      "Posts obtained: 1210\n",
      "Posts obtained: 1256\n",
      "Posts obtained: 1301\n",
      "Posts obtained: 1353\n",
      "Posts obtained: 1396\n",
      "Posts obtained: 1435\n",
      "Posts obtained: 1484\n",
      "Posts obtained: 1530\n",
      "Posts obtained: 1577\n",
      "Posts obtained: 1625\n",
      "Posts obtained: 1668\n",
      "Posts obtained: 1715\n",
      "Posts obtained: 1761\n",
      "Posts obtained: 1804\n",
      "Posts obtained: 1850\n",
      "Posts obtained: 1898\n",
      "Posts obtained: 1945\n",
      "Posts obtained: 1985\n",
      "Posts obtained: 2029\n",
      "Posts obtained: 2073\n",
      "Posts obtained: 2115\n",
      "Posts obtained: 2152\n",
      "Posts obtained: 2194\n",
      "Posts obtained: 2234\n",
      "Posts obtained: 2273\n",
      "Posts obtained: 2321\n",
      "Posts obtained: 2360\n",
      "Posts obtained: 2397\n",
      "Posts obtained: 2431\n",
      "Posts obtained: 2467\n",
      "Posts obtained: 2508\n",
      "Posts obtained: 2551\n",
      "Posts obtained: 2599\n",
      "Posts obtained: 2642\n",
      "Posts obtained: 2687\n",
      "Posts obtained: 2731\n",
      "Posts obtained: 2781\n",
      "Posts obtained: 2837\n",
      "Posts obtained: 2883\n",
      "Posts obtained: 2925\n",
      "Posts obtained: 2967\n",
      "Posts obtained: 3009\n",
      "Posts obtained: 3049\n",
      "Posts obtained: 3103\n",
      "Posts obtained: 3144\n",
      "Posts obtained: 3186\n",
      "Posts obtained: 3227\n",
      "Posts obtained: 3267\n",
      "Posts obtained: 3307\n",
      "Posts obtained: 3357\n",
      "Posts obtained: 3400\n",
      "Posts obtained: 3440\n",
      "Posts obtained: 3472\n",
      "Posts obtained: 3517\n",
      "Posts obtained: 3553\n",
      "Posts obtained: 3597\n",
      "Posts obtained: 3645\n",
      "Posts obtained: 3700\n",
      "Posts obtained: 3746\n",
      "Posts obtained: 3789\n",
      "Posts obtained: 3833\n",
      "Posts obtained: 3876\n",
      "Posts obtained: 3920\n",
      "Posts obtained: 3969\n",
      "Posts obtained: 4017\n",
      "Posts obtained: 4064\n",
      "Posts obtained: 4106\n",
      "Posts obtained: 4158\n",
      "Posts obtained: 4197\n",
      "Posts obtained: 4260\n",
      "Posts obtained: 4311\n",
      "Posts obtained: 4353\n",
      "Posts obtained: 4397\n",
      "Posts obtained: 4447\n",
      "Posts obtained: 4481\n",
      "Posts obtained: 4530\n",
      "Posts obtained: 4572\n",
      "Posts obtained: 4616\n",
      "Posts obtained: 4658\n",
      "Posts obtained: 4704\n",
      "Posts obtained: 4745\n",
      "Posts obtained: 4793\n",
      "Posts obtained: 4836\n",
      "Posts obtained: 4894\n",
      "Posts obtained: 4940\n",
      "Posts obtained: 4992\n",
      "Posts obtained: 5048\n",
      "Dump file scrapes_atheism.pickle saved\n",
      "----------------\n",
      "Running subreddit: christianity\n",
      "Posts obtained: 49\n",
      "Posts obtained: 106\n",
      "Posts obtained: 167\n",
      "Posts obtained: 220\n",
      "Posts obtained: 274\n",
      "Posts obtained: 324\n",
      "Posts obtained: 382\n",
      "Posts obtained: 434\n",
      "Posts obtained: 481\n",
      "Posts obtained: 535\n",
      "Posts obtained: 586\n",
      "Posts obtained: 640\n",
      "Posts obtained: 692\n",
      "Posts obtained: 738\n",
      "Posts obtained: 788\n",
      "Posts obtained: 845\n",
      "Posts obtained: 898\n",
      "Posts obtained: 948\n",
      "Posts obtained: 1007\n",
      "Posts obtained: 1063\n",
      "Posts obtained: 1111\n",
      "Posts obtained: 1170\n",
      "Posts obtained: 1218\n",
      "Posts obtained: 1265\n",
      "Posts obtained: 1324\n",
      "Posts obtained: 1378\n",
      "Posts obtained: 1435\n",
      "Posts obtained: 1480\n",
      "Posts obtained: 1526\n",
      "Posts obtained: 1577\n",
      "Posts obtained: 1639\n",
      "Posts obtained: 1683\n",
      "Posts obtained: 1741\n",
      "Posts obtained: 1801\n",
      "Posts obtained: 1863\n",
      "Posts obtained: 1918\n",
      "Posts obtained: 1968\n",
      "Posts obtained: 2024\n",
      "Posts obtained: 2078\n",
      "Posts obtained: 2136\n",
      "Posts obtained: 2189\n",
      "Posts obtained: 2243\n",
      "Posts obtained: 2291\n",
      "Posts obtained: 2341\n",
      "Posts obtained: 2383\n",
      "Posts obtained: 2427\n",
      "Posts obtained: 2482\n",
      "Posts obtained: 2537\n",
      "Posts obtained: 2596\n",
      "Posts obtained: 2653\n",
      "Posts obtained: 2712\n",
      "Posts obtained: 2766\n",
      "Posts obtained: 2804\n",
      "Posts obtained: 2871\n",
      "Posts obtained: 2914\n",
      "Posts obtained: 2971\n",
      "Posts obtained: 3026\n",
      "Posts obtained: 3077\n",
      "Posts obtained: 3131\n",
      "Posts obtained: 3176\n",
      "Posts obtained: 3223\n",
      "Posts obtained: 3280\n",
      "Posts obtained: 3331\n",
      "Posts obtained: 3389\n",
      "Posts obtained: 3443\n",
      "Posts obtained: 3494\n",
      "Posts obtained: 3553\n",
      "Posts obtained: 3614\n",
      "Posts obtained: 3662\n",
      "Posts obtained: 3711\n",
      "Posts obtained: 3768\n",
      "Posts obtained: 3823\n",
      "Posts obtained: 3878\n",
      "Posts obtained: 3932\n",
      "Posts obtained: 3987\n",
      "Posts obtained: 4041\n",
      "Posts obtained: 4092\n",
      "Posts obtained: 4142\n",
      "Posts obtained: 4187\n",
      "Posts obtained: 4238\n",
      "Posts obtained: 4286\n",
      "Posts obtained: 4328\n",
      "Posts obtained: 4381\n",
      "Posts obtained: 4424\n",
      "Posts obtained: 4482\n",
      "Posts obtained: 4540\n",
      "Posts obtained: 4593\n",
      "Posts obtained: 4663\n",
      "Posts obtained: 4716\n",
      "Posts obtained: 4770\n",
      "Posts obtained: 4828\n",
      "Posts obtained: 4884\n",
      "Posts obtained: 4940\n",
      "Posts obtained: 4991\n",
      "Posts obtained: 5041\n",
      "Dump file scrapes_christianity.pickle saved\n"
     ]
    }
   ],
   "source": [
    "#iterate through each subreddit topic and save to pickle file\n",
    "for sub in subreddit:\n",
    "    b_url=baseurl+sub+\"&before=\"\n",
    "    url=b_url\n",
    "    continue_flag=True\n",
    "    current_count=0\n",
    "    saved_scrapes=[]\n",
    "    \n",
    "    print(\"----------------\")\n",
    "    print(f\"Running subreddit: {sub}\")\n",
    "    \n",
    "    while continue_flag:\n",
    "        resp=requests.get(url)\n",
    "#         print(f\"Fetching url: {url}\")  #debug purpose only\n",
    "        js=json.loads(resp.text)\n",
    "        \n",
    "        current_count, continue_flag , url, saved_scrapes= check(js,current_count,continue_flag,b_url,saved_scrapes)\n",
    "        time.sleep(1) #adds a delay to prevent server from DDOS-ed\n",
    "    \n",
    "    scrape_path=f\"../data/scrapes_{sub}.pickle\"\n",
    "    with open(scrape_path,'wb') as handle:\n",
    "        pickle.dump(saved_scrapes,handle)\n",
    "    print(f\"Dump file scrapes_{sub}.pickle saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
