{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9864b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from classifiers import Classifiers\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9103c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_path_0=\"../scrape_file/scrapes_atheism.pickle\"\n",
    "scrape_path_1=\"../scrape_file/scrapes_christianity.pickle\"\n",
    "\n",
    "try:\n",
    "    with open(scrape_path_0,'rb') as handle:\n",
    "        scrape0=pickle.load(handle)\n",
    "    with open(scrape_path_1,'rb') as handle:\n",
    "        scrape1=pickle.load(handle)\n",
    "except FileNotFoundError as e:\n",
    "    e.strerror = \"Pls run 01_scrape_reddit first to pull the data.\"\n",
    "    raise e\n",
    "\n",
    "df0=pd.DataFrame(scrape0,columns=[\"post\"])\n",
    "df0['label']=0\n",
    "\n",
    "df1=pd.DataFrame(scrape1,columns=[\"post\"])\n",
    "df1['label']=1\n",
    "\n",
    "df=pd.concat([df0,df1])\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14178f3e",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0965f541",
   "metadata": {},
   "source": [
    "X=df['post']\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f42129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "regexptokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044d0500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [it, s, actually, just, sad, that, i, see, the...\n",
       "1        [tldr, i, love, my, christian, mother, but, sh...\n",
       "2        [i, literally, posted, here, like, an, hour, o...\n",
       "3        [my, wife, and, i, are, in, the, middle, of, l...\n",
       "4        [this, is, the, nail, in, the, coffin, for, th...\n",
       "                               ...                        \n",
       "10076    [jesus, lived, a, perfect, life, and, died, in...\n",
       "10077    [why, don, t, nuns, wear, the, blue, coat, red...\n",
       "10078    [i, ve, had, a, lack, of, appetite, for, somet...\n",
       "10079    [title, says, it, all, wondering, if, anyone, ...\n",
       "10080    [you, don, t, have, to, be, in, tennessee, if,...\n",
       "Name: post, Length: 10081, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post']=df['post'].apply(lambda sentence: regexptokenizer.tokenize(sentence.lower()))\n",
    "df['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb38060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [it, s, actually, just, sad, that, i, see, the...\n",
       "1        [tldr, i, love, my, christian, mother, but, sh...\n",
       "2        [i, literally, posted, here, like, an, hour, o...\n",
       "3        [my, wife, and, i, are, in, the, middle, of, l...\n",
       "4        [this, is, the, nail, in, the, coffin, for, th...\n",
       "                               ...                        \n",
       "10076    [jesus, lived, a, perfect, life, and, died, in...\n",
       "10077    [why, don, t, nun, wear, the, blue, coat, red,...\n",
       "10078    [i, ve, had, a, lack, of, appetite, for, somet...\n",
       "10079    [title, say, it, all, wondering, if, anyone, c...\n",
       "10080    [you, don, t, have, to, be, in, tennessee, if,...\n",
       "Name: post, Length: 10081, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post']=df['post'].apply(lambda sentence: [lemmatizer.lemmatize(word,pos='n') for word in sentence])\n",
    "df['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450cfc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [it, s, actually, just, sad, that, i, see, the...\n",
       "1        [tldr, i, love, my, christian, mother, but, sh...\n",
       "2        [i, literally, post, here, like, an, hour, or,...\n",
       "3        [my, wife, and, i, be, in, the, middle, of, lo...\n",
       "4        [this, be, the, nail, in, the, coffin, for, th...\n",
       "                               ...                        \n",
       "10076    [jesus, live, a, perfect, life, and, die, in, ...\n",
       "10077    [why, don, t, nun, wear, the, blue, coat, red,...\n",
       "10078    [i, ve, have, a, lack, of, appetite, for, some...\n",
       "10079    [title, say, it, all, wonder, if, anyone, can,...\n",
       "10080    [you, don, t, have, to, be, in, tennessee, if,...\n",
       "Name: post, Length: 10081, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post']=df['post'].apply(lambda sentence: [lemmatizer.lemmatize(word,pos='v') for word in sentence])\n",
    "df['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f866f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post']=df['post'].apply(lambda text: ' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75924f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['post']\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e770dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27345"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(stop_words=stopwords.words(\"english\"),token_pattern=\"[^\\W\\d_]+\")\n",
    "\n",
    "cv.fit_transform(X)\n",
    "\n",
    "arr_=cv.get_feature_names_out()\n",
    "#number of features\n",
    "len(arr_)\n",
    "# arr_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f0ad2",
   "metadata": {},
   "source": [
    "## Modify Pipeline for TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d6c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(cls):\n",
    "    return Pipeline([\n",
    "        ('tfidf',TfidfVectorizer(stop_words=stopwords.words(\"english\"),token_pattern=\"[^\\W\\d_]+\")),\n",
    "        ('cls',cls)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abcdd149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_obj=Classifiers(create_pipe,X,y,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe9d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[\n",
    "    {\n",
    "        'cls':RandomForestClassifier(n_jobs=-1),\n",
    "        'name':'Random Forest'\n",
    "    },\n",
    "#     {\n",
    "#         'cls':SVC(),\n",
    "#         'name':'SVC'\n",
    "#     },\n",
    "    {\n",
    "        'cls':xgb.XGBClassifier(objective=\"binary:logistic\"),\n",
    "        'name':'XGBoost'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2d5557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Running classifier: Random Forest =========\n",
      "--Train--\n",
      "Accuracy: 0.9995\n",
      "--Test--\n",
      "Accuracy: 0.8146\n",
      "Precision: 0.8095\n",
      "Recall: 0.8239\n",
      "Specificity: 0.8052\n",
      "F1: 0.8167\n",
      "---Other metrics regarding Random Forest---\n",
      "Average tree depth: 234.87\n",
      "========= Running classifier: XGBoost =========\n",
      "--Train--\n",
      "Accuracy: 0.9499\n",
      "--Test--\n",
      "Accuracy: 0.8225\n",
      "Precision: 0.8179\n",
      "Recall: 0.8309\n",
      "Specificity: 0.8141\n",
      "F1: 0.8243\n"
     ]
    }
   ],
   "source": [
    "for i in classifiers:\n",
    "    class_obj.run_classifier(i['cls'],i['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d6bb6",
   "metadata": {},
   "source": [
    "Looks like TF-IDF doesn't help much either. Let's look at other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f85799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
